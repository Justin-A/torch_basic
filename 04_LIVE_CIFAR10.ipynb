{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding = \"bytes\")\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "tmp = unpickle(\"../data/CIFAR10/cifar-10-batches-py/data_batch_1\")\n",
    "print(tmp.keys())\n",
    "print(tmp[b'data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_data(root):\n",
    "    data = unpickle(root)\n",
    "    data_images = data[b\"data\"] / 255\n",
    "    data_images = data_images.reshape(-1, 3, 32, 32).astype(\"float32\")\n",
    "    data_labels = data[b\"labels\"]\n",
    "    return data_images, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images1, label1 = batch_to_data(\"../data/CIFAR10/cifar-10-batches-py/data_batch_1\")\n",
    "images2, label2 = batch_to_data(\"../data/CIFAR10/cifar-10-batches-py/data_batch_2\")\n",
    "images3, label3 = batch_to_data(\"../data/CIFAR10/cifar-10-batches-py/data_batch_3\")\n",
    "images4, label4 = batch_to_data(\"../data/CIFAR10/cifar-10-batches-py/data_batch_4\")\n",
    "images5, label5 = batch_to_data(\"../data/CIFAR10/cifar-10-batches-py/data_batch_5\")\n",
    "test_images, test_labels = batch_to_data(\"../data/CIFAR10/cifar-10-batches-py/test_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_images = np.concatenate([images1, images2, images3, images4, images5], axis = 0)\n",
    "train_labels = np.concatenate([label1, label2, label3, label4, label5], axis = 0)\n",
    "test_labels = np.concatenate([test_labels], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train, Validation Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_images, valid_images, train_labels, valid_labels = train_test_split(train_images, train_labels, stratify = train_labels, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of Train Images:  (40000, 3, 32, 32)\n",
      "The Shape of Valid Images:  (10000, 3, 32, 32)\n",
      "The Shape of Test Images:  (10000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Check Images\n",
    "print(\"The Shape of Train Images: \", train_images.shape)\n",
    "print(\"The Shape of Valid Images: \", valid_images.shape)\n",
    "print(\"The Shape of Test Images: \", test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number of Train Labels:  (40000,)\n",
      "The Number of Valid Labels:  (10000,)\n",
      "The Number of Test Labels:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Check Labels\n",
    "print(\"The Number of Train Labels: \", train_labels.shape)\n",
    "print(\"The Number of Valid Labels: \", valid_labels.shape)\n",
    "print(\"The Number of Test Labels: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataLoader with TensorDataset\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_images_tensor = torch.tensor(train_images)\n",
    "train_labels_tensor = torch.tensor(train_labels)\n",
    "train_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "train_loader = DataLoader(train_tensor, batch_size = 64, num_workers = 0, shuffle = True)\n",
    "\n",
    "valid_images_tensor = torch.tensor(valid_images)\n",
    "valid_labels_tensor = torch.tensor(valid_labels)\n",
    "valid_tensor = TensorDataset(valid_images_tensor, valid_labels_tensor)\n",
    "valid_loader = DataLoader(valid_tensor, batch_size = 64, num_workers = 0, shuffle = True)\n",
    "\n",
    "test_images_tensor = torch.tensor(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make CNN Architecture\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, stride = 1, padding = 1, padding_mode = \"zeros\")\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, stride = 1, padding = 1, padding_mode = \"zeros\")\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 16, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # 32 * 32 * 3\n",
    "        x = F.relu(x)     # 32 * 32 * 3\n",
    "        x = self.pool(x)  # 16 * 16 * 3\n",
    "        x = self.conv2(x) # 16 * 16 * 16\n",
    "        x = F.relu(x)     # 16 * 16 * 16\n",
    "        x = self.pool(x)  #  8 *  8 * 16\n",
    "        x = x.view(-1, 8 * 8 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  CNN(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "model = CNN().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "print(\"Model: \", model)\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Train & Evaluate\n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE, dtype = torch.int64)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE, dtype = torch.int64)\n",
    "            output = model(data)\n",
    "            valid_loss += F.cross_entropy(output, target, reduction = \"sum\").item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    valid_accuracy = 100. * correct / len(valid_loader.dataset)\n",
    "    return valid_loss, valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\venv\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.309410\n",
      "Train Epoch: 1 [6400/40000 (16%)]\tLoss: 2.074328\n",
      "Train Epoch: 1 [12800/40000 (32%)]\tLoss: 1.949840\n",
      "Train Epoch: 1 [19200/40000 (48%)]\tLoss: 1.804405\n",
      "Train Epoch: 1 [25600/40000 (64%)]\tLoss: 1.731735\n",
      "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.595117\n",
      "Train Epoch: 1 [38400/40000 (96%)]\tLoss: 1.696486\n",
      "[EPOCH: 1], \tValidation Loss: 1.5815, \tValidation Accuracy: 42.64 %\n",
      "\n",
      "Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.603365\n",
      "Train Epoch: 2 [6400/40000 (16%)]\tLoss: 1.544948\n",
      "Train Epoch: 2 [12800/40000 (32%)]\tLoss: 1.324980\n",
      "Train Epoch: 2 [19200/40000 (48%)]\tLoss: 1.442722\n",
      "Train Epoch: 2 [25600/40000 (64%)]\tLoss: 1.641744\n",
      "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.755652\n",
      "Train Epoch: 2 [38400/40000 (96%)]\tLoss: 1.377651\n",
      "[EPOCH: 2], \tValidation Loss: 1.4904, \tValidation Accuracy: 45.53 %\n",
      "\n",
      "Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.609147\n",
      "Train Epoch: 3 [6400/40000 (16%)]\tLoss: 1.512689\n",
      "Train Epoch: 3 [12800/40000 (32%)]\tLoss: 1.594874\n",
      "Train Epoch: 3 [19200/40000 (48%)]\tLoss: 1.428402\n",
      "Train Epoch: 3 [25600/40000 (64%)]\tLoss: 1.534380\n",
      "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.373245\n",
      "Train Epoch: 3 [38400/40000 (96%)]\tLoss: 1.335650\n",
      "[EPOCH: 3], \tValidation Loss: 1.3472, \tValidation Accuracy: 51.43 %\n",
      "\n",
      "Train Epoch: 4 [0/40000 (0%)]\tLoss: 1.270502\n",
      "Train Epoch: 4 [6400/40000 (16%)]\tLoss: 1.321063\n",
      "Train Epoch: 4 [12800/40000 (32%)]\tLoss: 1.377097\n",
      "Train Epoch: 4 [19200/40000 (48%)]\tLoss: 1.515341\n",
      "Train Epoch: 4 [25600/40000 (64%)]\tLoss: 1.323171\n",
      "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 1.391142\n",
      "Train Epoch: 4 [38400/40000 (96%)]\tLoss: 1.353428\n",
      "[EPOCH: 4], \tValidation Loss: 1.3010, \tValidation Accuracy: 53.31 %\n",
      "\n",
      "Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.039035\n",
      "Train Epoch: 5 [6400/40000 (16%)]\tLoss: 1.515554\n",
      "Train Epoch: 5 [12800/40000 (32%)]\tLoss: 1.200045\n",
      "Train Epoch: 5 [19200/40000 (48%)]\tLoss: 1.008698\n",
      "Train Epoch: 5 [25600/40000 (64%)]\tLoss: 1.212051\n",
      "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.139057\n",
      "Train Epoch: 5 [38400/40000 (96%)]\tLoss: 1.387611\n",
      "[EPOCH: 5], \tValidation Loss: 1.2817, \tValidation Accuracy: 54.12 %\n",
      "\n",
      "Train Epoch: 6 [0/40000 (0%)]\tLoss: 1.205688\n",
      "Train Epoch: 6 [6400/40000 (16%)]\tLoss: 1.142613\n",
      "Train Epoch: 6 [12800/40000 (32%)]\tLoss: 1.274953\n",
      "Train Epoch: 6 [19200/40000 (48%)]\tLoss: 1.100997\n",
      "Train Epoch: 6 [25600/40000 (64%)]\tLoss: 1.057582\n",
      "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 1.200381\n",
      "Train Epoch: 6 [38400/40000 (96%)]\tLoss: 1.292703\n",
      "[EPOCH: 6], \tValidation Loss: 1.2466, \tValidation Accuracy: 56.04 %\n",
      "\n",
      "Train Epoch: 7 [0/40000 (0%)]\tLoss: 1.264152\n",
      "Train Epoch: 7 [6400/40000 (16%)]\tLoss: 1.155693\n",
      "Train Epoch: 7 [12800/40000 (32%)]\tLoss: 1.255478\n",
      "Train Epoch: 7 [19200/40000 (48%)]\tLoss: 1.029585\n",
      "Train Epoch: 7 [25600/40000 (64%)]\tLoss: 1.395509\n",
      "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 1.105530\n",
      "Train Epoch: 7 [38400/40000 (96%)]\tLoss: 1.130941\n",
      "[EPOCH: 7], \tValidation Loss: 1.2224, \tValidation Accuracy: 56.44 %\n",
      "\n",
      "Train Epoch: 8 [0/40000 (0%)]\tLoss: 1.239014\n",
      "Train Epoch: 8 [6400/40000 (16%)]\tLoss: 1.165502\n",
      "Train Epoch: 8 [12800/40000 (32%)]\tLoss: 1.072052\n",
      "Train Epoch: 8 [19200/40000 (48%)]\tLoss: 1.089980\n",
      "Train Epoch: 8 [25600/40000 (64%)]\tLoss: 1.357719\n",
      "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.996899\n",
      "Train Epoch: 8 [38400/40000 (96%)]\tLoss: 1.117524\n",
      "[EPOCH: 8], \tValidation Loss: 1.1889, \tValidation Accuracy: 57.96 %\n",
      "\n",
      "Train Epoch: 9 [0/40000 (0%)]\tLoss: 1.252936\n",
      "Train Epoch: 9 [6400/40000 (16%)]\tLoss: 0.999543\n",
      "Train Epoch: 9 [12800/40000 (32%)]\tLoss: 1.055719\n",
      "Train Epoch: 9 [19200/40000 (48%)]\tLoss: 1.058174\n",
      "Train Epoch: 9 [25600/40000 (64%)]\tLoss: 1.222244\n",
      "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 1.141698\n",
      "Train Epoch: 9 [38400/40000 (96%)]\tLoss: 0.947335\n",
      "[EPOCH: 9], \tValidation Loss: 1.1488, \tValidation Accuracy: 59.67 %\n",
      "\n",
      "Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.945311\n",
      "Train Epoch: 10 [6400/40000 (16%)]\tLoss: 1.038990\n",
      "Train Epoch: 10 [12800/40000 (32%)]\tLoss: 1.161929\n",
      "Train Epoch: 10 [19200/40000 (48%)]\tLoss: 1.133112\n",
      "Train Epoch: 10 [25600/40000 (64%)]\tLoss: 0.928177\n",
      "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.751176\n",
      "Train Epoch: 10 [38400/40000 (96%)]\tLoss: 0.849691\n",
      "[EPOCH: 10], \tValidation Loss: 1.1579, \tValidation Accuracy: 59.01 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''TRAINING'''\n",
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer)\n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_loader)\n",
    "    print(\"[EPOCH: {}], \\tValidation Loss: {:.4f}, \\tValidation Accuracy: {:.2f} %\\n\".format(\n",
    "        epoch, valid_loss, valid_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testset_prediction(model, test_images_tensor):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_images_tensor:\n",
    "            data = data.to(DEVICE)\n",
    "            output = model(data.view(-1, 3, 32, 32))\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            result.append(prediction.tolist())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\venv\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5916"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test_predict_result = testset_prediction(model, test_images_tensor)\n",
    "accuracy_score(test_labels, np.squeeze(test_predict_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAfJUlEQVR4nO2dW4xk13We/1X3S997pi/TcyU1UkhdSBEDQokSQbETgxEMUAJiQ3oQ+CB4jMACIsB5IBQgUoA8yEEkQU8KRhFhOlB0iSVBRCAkJgjbjA2YEkmTQ1Ikh+RoOLeevt+quu618tBFY8jsf3dzurt6xPN/wKBr9qp9zjr7nHVO1f5rrW3uDiHEe5/UQTsghOgPCnYhEoKCXYiEoGAXIiEo2IVICAp2IRJCZjedzewBAN8CkAbw39z9a7H3Z9Pm+YyFHclmab9MJuymd7q0T7vVpLaUhX0AgFw2MiREpux0O5EuXNpMpfi9Nmbj3gNGji2VTvN9pfm+Yspst8vHn++Pex/bXswWOZ0U99i+bvWccUfYeQGAVrsdbK83wu1bGwyPb73ZRrPdCe7MblVnN7M0gAsA/iWAqwB+CeBz7v4r1mcgn/KPTIWD+vDRo3Rf4+PjwfbmRpX2Wbx6idoGCnlqm5mYoDZvt4Lt65UV2qfdDfcBgFKhSG0DxRK1ZSOBa+TGODA0RPsUB8vU1orcyDZrDWorl0eC7RYJls3aJrVVNvm5zpAHCACkSQDWG3Xap9ngx1WMnJdiiV9X2YiPNxaWgu2v/nqR9vFM+Hz+4sINrG82gjvbzcf4+wG87u4X3b0J4AcAHtzF9oQQ+8hugn0GwJWb/n+11yaEuA3ZzXf20EeF/+87gZmdBXAWAHL806cQYp/ZzZP9KoBjN/3/KIDr73yTu59z9zPufiabvoWZFCHEnrCbYP8lgNNmdsrMcgA+C+CxvXFLCLHX3PLHeHdvm9kXAfwfbElvj7j7S7E+aTMM58Oz8dNjfLa43gzPnGYzXEkYGIrMZmcislaB29qt8CeTgdwg31cuR22Dg+EZawCwdkTOI+MBAJlceEZ45NAh2gcR2XNokB9baq1CbbncQHhXEZkvld+gNotIoumIOtEi8mzKeZ9SgV+L+cj5LBX5OMYeq9VO+LiXNvkn4dX15WB7o8nlul3p7O7+cwA/3802hBD9Qb+gEyIhKNiFSAgKdiESgoJdiISgYBciIexqNv7dkjJHOR3ORvNKWEoAgMnh0WB7ZYMnmazUuCxUGOCJH7VIv5HRsCST6nLJxdIFaquD98uXuY9N58fdaIcTRrqrc7RPNyJDlWtc5rMUv3w6Hh7HOkkmAoBMRF47PDVJbbVNnkDj1fB4jI1w2bNU4rJtPs+TXeqbXDq8vsiTpdbq4fEfmTxO+1yauxBs78SyLKlFCPGeQsEuREJQsAuREBTsQiQEBbsQCaGvs/H5bAanjoQTMtwiiR8bZKZ+dZ32mYrVA4skC/h6jdrWNsNKQjeS3LHaWKW2jchs/OFpXh6rXeWKwZGhcKJGk8xKA0CrxWdw1xb4LDIiJabSubDNI7XY8sVw8gwADHd4Ik+twmfj6/S4I+XYRvlMfWWN1667dv0GtT1/4Rq1VVNhlcczXMnpEFUjWvOQWoQQ7ykU7EIkBAW7EAlBwS5EQlCwC5EQFOxCJIS+Sm+FYhF3ffDDQVunGlkNpBKW2Bpl7n42knCxWOOJH6/Pcz82LSy7dCMS1GKVry6SG+bJLpXrXLLLNPjSVnfPnA62Tw7y1WeakZp2G5FVWmqRJZnqrfD41+tc2qxFxqrVnKe2Qp6PYz4blvM6zsdwaYWP/fx8ePUWALh+g0vB1xf5GHsxnFzT2OTJYWPk2q+vcwlbT3YhEoKCXYiEoGAXIiEo2IVICAp2IRKCgl2IhLAr6c3MLgHYANAB0Hb3M7H3O4AmkamGIjXByoVwJlenw7Ok6pE1JJ8/f4XaXlpcozYrhSWSTKQW2+DgOHcky+uZLS1xiccaXKL6v8++EWw/fYL7cepouMYfAByeOEptAwPT1NYmi3hWI3XauhE5bH5hlto2YzXo0uEssGZENlxc5VmFV+e4LLe0yv1oNrkkVqksBtuLBS4Rs2XFYkun7oXO/s/dPeytEOK2QR/jhUgIuw12B/AXZvaMmZ3dC4eEEPvDbj/Gf9zdr5vZBIDHzewVd3/y5jf0bgJnAWBskFfeEELsL7t6srv79d7feQA/BXB/4D3n3P2Mu58ZKPK1rYUQ+8stB7uZlc1s8K3XAH4HwIt75ZgQYm/Zzcf4SQA/ta0CghkA/8Pd/3esQ6PVwsXZsIQyM8LlnxMj4aymdJffq5bqvPDe0irPvEqRLCkAKJElmdYihS874HLMYJpLK5MTfLmjhRsL1Pb6XLhA5FyFS3lzG2PU9on7/gm1TQ/yfm2Ei3oW87zI5npkCbBCjverVrhkt7EZLjjJ2gFgcY3bqo3I0lsROa9Q5NfqxFT42s+k+DVc3QhLmJF6nrce7O5+EcA9t9pfCNFfJL0JkRAU7EIkBAW7EAlBwS5EQlCwC5EQ+lpw0gG0u2E54ZnzL/OO758JNn/gOM/k6m5wOaZW5/LJeoWvA5clhQELOZ69Vo0UbOx2uATYLJaobXg4vDYYAGQKYYmq0eAFDy9c51lexcIrfF9pLjWl2mH5qus8+6vV5OesW+NyWCnNz2fHwsdddT4erTrfVzviY7HMfzRWMC6jnTrBpDfaBfNz4ev0ygY/l3qyC5EQFOxCJAQFuxAJQcEuREJQsAuREPo6G59OpTE8FJ55rI7wGdUXL4aTZ6bG+LI/hTy/jw2N8KWQRsBnTVse9nF0mCfPNJq8Xlyzxmf+axVe62x0jB/b6PhwsH2Vl9ZDfZOrAn9/IVzTDgCaLT5r/ZGTh4PtpQwfX5DxBYBMhs/id5yrApksGatIxgjfGpDJ85CptPixtTvc/0Y9rAzkSc1DACjkwn6kIselJ7sQCUHBLkRCULALkRAU7EIkBAW7EAlBwS5EQuir9AYHvBUWNmZO3Um7vfHqxWD73z7za9rnvg/fQW2jI4Pc1uWJDqvrYf3Ku1wyGonsq1rh/SLlzICI1LSyFK7jNjQUluQAIJ/hl8H8PK9398yr16lteTk8Vu+b4bUGJ0d5Ioml+DHXW3wcK43w+azU+HlutPm+umT5MgDIRurkDRe53Lu5Gfa/WePJOk6kPI8om3qyC5EQFOxCJAQFuxAJQcEuREJQsAuREBTsQiSEbaU3M3sEwO8CmHf3D/XaxgD8EMBJAJcA/L67h9cduolGo443LlwI2kaOc6kMpXBW2ZuXuSzU/NU1atto8cyg9XW+lNPoYDjLLpbZNjI+QW0Dg1ziiS0pVSC18ABgoxrOYLt+I5w5CACHJ7iPhyfD2WsAsBpJpXtjIbw80VqdS15HD/G6e5OjXLpKpXj2YL0ZtrVjSzXluQRYLHM/8gNc3kxFJLu1lbBcWqnyrMIckfk8krW5kyf7nwJ44B1tDwN4wt1PA3ii938hxG3MtsHeW2/9nbeeBwE82nv9KIBP77FfQog95la/s0+6+ywA9P7yz4FCiNuCff+5rJmdBXAWAMo5zQcKcVDcavTNmdk0APT+zrM3uvs5dz/j7mfyrESQEGLfudXoewzAQ73XDwH42d64I4TYL3YivX0fwCcBHDKzqwC+AuBrAH5kZl8AcBnA7+1kZ4PlEj7xj+8N2p56hUtlk8fCGXEnZ/hUwd/+9VPUtrDBs4ksIru4pYPtm5HCkc35JWobGuYFM1NpXqAwnebSYYZIMo21sBQGANeu8+y1cpkX05yYnqK2GpHYllYWaZ/1K1y9XVjl/p+YHqG2LsmIc/AxvOeeD1Pb9JHj1Da/wqXI67Nc+qxuhJ+5+VKB9jFyLcL483vbYHf3zxHTb2/XVwhx+6Av0UIkBAW7EAlBwS5EQlCwC5EQFOxCJIS+FpzMZjOYnhwP2savcomqXQtLGjOneabch+75ELU9/uTT1FYucDmsQoplVhs8g8rqfB21TpdLdtkMkVYA1CKFCBvNsGRXLPFsrW5kHbJag+/rRiSTbmpqMtieiWTYLS9yWW65yotKDqxzHwcKYYltamqa9hkfC1+jAHDq5Aluu5OH0ysk2xMA5uduBNtjkm4mw57Tu8t6E0K8B1CwC5EQFOxCJAQFuxAJQcEuREJQsAuREPoqvXUc2OiGpYHhES4NzS2Hs6FadV6Q7wPvm6G2N6+FpQ4AuHSDZ1flh8IZcak8v2emunz9r27kXlso8myzygb30btEesvxbL5sjsuNXefZYbWIrMjWiJs+ws/LnaffT22sUCkAXF3mRSxnJsLjeHyEF9LsRI55dZ1fc6PjPAvwyJFj1DYzHZYjX3ttlfYp58PxkjLuu57sQiQEBbsQCUHBLkRCULALkRAU7EIkhL7OxjfaHVycqwRt5dFDtF++Hk6CWN7gM6P5Al9KCCk+Y3ntCp+pP3wkPIM7NjpG+9Q2+UxxOuJHu82TUwYG+Ex9qxNOnsgVeD2zRoMnmcSWLRof5+dseTm8pNGbb16mfY4f5zPWA0ND1DY/R4sbo9sl11uJJ93Erp2XX+OqQO5NXkfx1CmeQDN+KHxdXbnMt5fLhJcAM83GCyEU7EIkBAW7EAlBwS5EQlCwC5EQFOxCJISdLP/0CIDfBTDv7h/qtX0VwB8AeCvb4cvu/vPttlWrt/Dcq2E5YajI67hVK2H5pHWdJwrUSC02AKis80SSYo4PycL1sJwEG6V9hkYGqS0XqTPXbvK6aineDRkL+x+TZGL16VIZnsgTk6jcwwlPrRaXyS5dukRtx4/zZZcOHY7UtZsLJ+S8eSXcDgBjY1xKTWX4ddVc4ctXsdWaAODK1fCYzM/z67RcCkus7TaPo5082f8UwAOB9m+6+729f9sGuhDiYNk22N39SQDkkSaE+E1hN9/Zv2hm583sEbPI51ghxG3BrQb7twHcCeBeALMAvs7eaGZnzexpM3u60eR1sIUQ+8stBbu7z7l7x927AL4D4P7Ie8+5+xl3P5OPTH4JIfaXWwp2M7t5OY3PAHhxb9wRQuwXO5Hevg/gkwAOmdlVAF8B8Ekzuxdba81cAvCHO9qbGZANZ+s02mF5DQBuzIUljcWlTdrn5J281tn7jvNsrVSDmvDChXBGXIdkmgFAschrv+VzXNZaqoaXvNrqx2vGuYUlr/X1ddpnaHiE2jqRpaHqdS4PDpEstdj2Zmf5clKxzLaZYzxbbmM9fF3dWODj8XdPvUBtxUH+fCxHJMzLl7nUV1kLZ2/mi/w83/XB08H2Cwv8ubttsLv75wLN392unxDi9kK/oBMiISjYhUgICnYhEoKCXYiEoGAXIiH0+VcuDvOw9GKR204hG5avShkueX3gOF+Kp5iJFIFschlqeS2chZTOcwktkmyGXJ7LcuksPzWdNj/uVDrsS7PJj3nuxhy1OSIFDCMZcaOj4V9QlyLyFJPrAGBtjUtlcwtclhsaDW9zqcbHox75oWe6zc/L+g2ehemRbXo3nKk2PcWvxa6F44hJr4Ce7EIkBgW7EAlBwS5EQlCwC5EQFOxCJAQFuxAJoa/SW9oMQ9nw/WVjk0sGE5OTwXZv8bWwvMGz6DIpLhmNDfEh+WdnjgbbX7nCJZd2i0s8TQ9nAAJAKst9bFb4NhvtWrA9m+HH1W3ztd5akYIjucizoroRztrb3OTnJZ3m2yuVeQbY2hrPEBw/NB5sHxrjhUA3Vvn5zDa5XNpq86qSqTS/vkcPhX3Jl/g5u3b1StiHiMSqJ7sQCUHBLkRCULALkRAU7EIkBAW7EAmhr7Px7VYLy3PhOmPNyAx5PR12sx1JMtls8PpouTSvg+ZdXoRudDA8ez5a5sM4u8nr5FUq/ADSJPkHADbb4ZplANAmSTIeycip1/kxu/N+LIEDAFg+RjNSt64befZ4JFMql+FjVSEJNOUSX7qqUOTJOvUGHysjS28BwGbkOsikwwrF0dHDtE+3Fb4GUlAijBCJR8EuREJQsAuREBTsQiQEBbsQCUHBLkRC2MnyT8cA/BmAKQBdAOfc/VtmNgbghwBOYmsJqN939/A6TT063TYqlfBbutkB2q80EE4UKA3xZIbB8XDyDACMD3CZr7rC67EVyXJNU5GkirUWT/ywfIHauhEZJ1viMk53MyyHNRqRunUpnsDhzqWcViu2zbBk12zxpJt0pKZd17jMl48k+dTr4cSgWkQKGx0do7bFxUVqa0eShoZJTT4AyGfDx5biQ4/3nwgvefV3b/Dj2smTvQ3gj939LgAfA/BHZnY3gIcBPOHupwE80fu/EOI2Zdtgd/dZd3+293oDwMsAZgA8CODR3tseBfDp/XJSCLF73tV3djM7CeCjAJ4CMOnus8DWDQHAxF47J4TYO3b8c1kzGwDwYwBfcvd1ixVEf3u/swDOAkAhu7M+Qoi9Z0dPdjPLYivQv+fuP+k1z5nZdM8+DSBYqd/dz7n7GXc/k80o2IU4KLYNdtt6hH8XwMvu/o2bTI8BeKj3+iEAP9t794QQe8VOPsZ/HMDnAbxgZs/12r4M4GsAfmRmXwBwGcDvbbehVCqNAqklNr8clkgAwBCWJkZGeF2yhSUuQRw/fIraBiPLLo0NhKUyyyzTPldW+HHVI0srtWMZZXnuYykVzuYyRLIAI1+v4hlx1IQWyQ7LROS16L5iXxszkbHqhp1sRvZVidTJKw7wa25jaYnaUjnuvxXC11UjknFYJHJ0Ks1l1G2D3d3/BqBX5W9v118IcXugX9AJkRAU7EIkBAW7EAlBwS5EQlCwC5EQ+lpw0iyFdDYsGUxODdN+jdpGsD2X41ljK5ElgS7P3qC2u4+ElwsCgOFy2MdCgReA7HZ5llc3kjWWK/AswEakOGeXFBzMxYoobnJ5sFjgl0i5GMnaI7JchxTEBIBCRFJsRy7VRiTbLE+kqFqkcGStwsdjYopnUxbI0mZAPFtuLBsuLFlrRgqjtsPLPMWyFPVkFyIhKNiFSAgKdiESgoJdiISgYBciISjYhUgIfZXeAH53mZmZpn1yhRPB9rVVLq91EZbrAGC5wrPUkOLra7HEqxvLfHv1Ns/ISuf48FcrPGuv0+aSTKsTll66kT7ZHM+UGiiE17cDgNomlxyRC2ffDQ0O0S6VDX7OWnU+jvTEAAApfBmrrNCJSHmbazwjbnCUy6VHj4ULRALAkZmwrbl0jfbJdsPSmzkfJz3ZhUgICnYhEoKCXYiEoGAXIiEo2IVICH2djc9lMzhKEk3SkTpio0Phul85MtMKAPUGn0Ver/JZ/MXIjPDxo4eC7WtVXt+t0+F+5NJ8+IeHebJLs8Xv0d4N26rr/LiGyhEfM3ymfjOSqDEycTS8vUiNtLVqeIYZANJpbisYT/4ol8LXTibNx3BlZZXa2HJSAGCRxc/yZX6uZxeuEkciS02NToUNEdFCT3YhEoKCXYiEoGAXIiEo2IVICAp2IRKCgl2IhLCt9GZmxwD8GYApbE3sn3P3b5nZVwH8AYCF3lu/7O4/j+4sk8bhsZGgzY1LMkYUnnykTzZSH229xuWTZ1+6yLdJpLKV9YgsVArX3NuyhZNFAKASSTLJRI7bibRVLuZon1SXJ37AuAQ4MT1DbfVWeExWNnjtt9WNSGIN2R4AZDOR2nWdsP/VyPg6IolGpPYbABRSvM4fSB1FAPBGWOqbGODXx/BIOKEoneHP753o7G0Af+zuz5rZIIBnzOzxnu2b7v5fdrANIcQBs5O13mYBzPZeb5jZywD4LV0IcVvyrr6zm9lJAB8F8FSv6Ytmdt7MHjGz0T32TQixh+w42M1sAMCPAXzJ3dcBfBvAnQDuxdaT/+uk31kze9rMnq7W+PcdIcT+sqNgN7MstgL9e+7+EwBw9zl377h7F8B3ANwf6uvu59z9jLufiU0SCSH2l22D3cwMwHcBvOzu37ip/eY6Up8B8OLeuyeE2Ct2Mhv/cQCfB/CCmT3Xa/sygM+Z2b0AHMAlAH+43YYMhoyF7y/FEq9NViiGbdWNddpnY3OJ2jK5cCYUANxYmqW2CxfDteY2G/yeWR6OSG9lLg8uLnL/N1a5bDRIJJnhMv9U1drk47i4zGvhtcBlOVYjLZPnY5+NZMTl87y+GyJ11yqVsOTVbPJMxXykNmAqki1X7/Bt3jHGZbmPzoSXlBoa5mNVHAqfT4v4t5PZ+L9BuD5fVFMXQtxe6Bd0QiQEBbsQCUHBLkRCULALkRAU7EIkhL4WnGy321hYDMtXY2PclUw6LFHVmlyCWo5kti3N81/yTUzxX/1OHB8Lts9VuXQ1MMDltdiyRZ0Wz0SbX+CFCOfJ+B4a49LmxGFuy+Z4McfKGi/M2GULLJHlqQCgmOK2fIkXxczmuASYr4VtzUj22tAAl/m8zX2sR67H4TK/rt53LFw8stngsifzIraslZ7sQiQEBbsQCUHBLkRCULALkRAU7EIkBAW7EAmhr9Jbp+uo1sOSUjFSULC+GM5EW9vgEpRluBzTiKzldeKuw9Q2SSSq116PFL6MyEkr63xxsFaXF2acOjZBbebhbKjqGi94ePnNG9Q2OsSlw7JFCj16WARqRAqYNOvcFimJiUKJZ5S12q1g+8AwL+ZYjhQrrS3zdQJHuzz7rpznWYcvXQ+v9VaKFNI8XgxXhrPI81tPdiESgoJdiISgYBciISjYhUgICnYhEoKCXYiE0FfpLZvNYnoqnOETK5S3QbLK5pe5hNbpcqkjnecZVGsVLpU9e/5SsH15nctkNeOZUOubPKtp5DCX1wZGwuvlAUCzEZaalm7w8W1VeK5UKSIZNSOZhax4ZD2SbdYmMhkA5JsRKRWR7LB0+NgGSryYY6rFz+epaZ699o8Oh7MiAaDufJu/mg1Ly8U0lxRz+bAfrTaX//RkFyIhKNiFSAgKdiESgoJdiISgYBciIWw7G29mBQBPAsj33v/n7v4VMzsF4AcAxgA8C+Dz7h5dprXTamJ57nLQtlblM7stD8+Qd53P0C5t8NQJcz7r6wsRG1neJ5/hiTC1Vb6MUz6iQAxFZmKLkWWGsuXwKT18jM8iV3g+DjKRpZWa+Ug9NjKMxRKf3e84P2e5yBjnSzyppdbuhPe1yS/VfOQyvusDR6ltOKJc/Hq+Qm3ZfDjBqg1+zFfWwipPs7O72fgGgN9y93uwtTzzA2b2MQB/AuCb7n4awAqAL+xgW0KIA2LbYPct3rotZXv/HMBvAfjzXvujAD69Lx4KIfaEna7Pnu6t4DoP4HEAbwBYdf+Hz11XAYQTbIUQtwU7CnZ377j7vQCOArgfwF2ht4X6mtlZM3vazJ6uNWMlCIQQ+8m7mo1391UAfwXgYwBGzP6hVMlRANdJn3PufsbdzxQj614LIfaXbYPdzA6b2UjvdRHAvwDwMoC/BPCve297CMDP9stJIcTu2cmjdhrAo2aWxtbN4Ufu/r/M7FcAfmBm/wnA3wP47nYbSqU6KGXDtdC6BS4Z1Frhe9LIAE8IcfAElBMzXIYaLfAkmU6VJN50uGyYyfKEi0yKS4elLPcD3bCcBAAdsgBQdpAvadQe5JIRItJbPbJE1eWV8HlerPLz4i0uNTmR0ACgVYtssx72v5Di+/rgHceo7cQhfu2sVHmdv9FxXttwvBi+RlYjx5Uvh+vkpbL8+b1tsLv7eQAfDbRfxNb3dyHEbwD6BZ0QCUHBLkRCULALkRAU7EIkBAW7EAnBnGSU7cvOzBYAvNn77yEAfP2m/iE/3o78eDu/aX6ccPegztfXYH/bjs2edvczB7Jz+SE/EuiHPsYLkRAU7EIkhIMM9nMHuO+bkR9vR368nfeMHwf2nV0I0V/0MV6IhHAgwW5mD5jZq2b2upk9fBA+9Py4ZGYvmNlzZvZ0H/f7iJnNm9mLN7WNmdnjZvZa7y9Pr9pfP75qZtd6Y/KcmX2qD34cM7O/NLOXzewlM/u3vfa+jknEj76OiZkVzOwXZvZ8z4//2Gs/ZWZP9cbjh2YWSVcM4O59/Qcgja2yVncAyAF4HsDd/faj58slAIcOYL+fAHAfgBdvavvPAB7uvX4YwJ8ckB9fBfDv+jwe0wDu670eBHABwN39HpOIH30dEwAGYKD3OgvgKWwVjPkRgM/22v8rgH/zbrZ7EE/2+wG87u4Xfav09A8APHgAfhwY7v4kgOV3ND+IrcKdQJ8KeBI/+o67z7r7s73XG9gqjjKDPo9JxI++4lvseZHXgwj2GQBXbvr/QRardAB/YWbPmNnZA/LhLSbdfRbYuugA8GVc958vmtn53sf8ff86cTNmdhJb9ROewgGOyTv8APo8JvtR5PUggj1USuWgJIGPu/t9AP4VgD8ys08ckB+3E98GcCe21giYBfD1fu3YzAYA/BjAl9w9vE73wfjR9zHxXRR5ZRxEsF8FcHPdH1qscr9x9+u9v/MAfoqDrbwzZ2bTAND7O38QTrj7XO9C6wL4Dvo0JmaWxVaAfc/df9Jr7vuYhPw4qDHp7ftdF3llHESw/xLA6d7MYg7AZwE81m8nzKxsZoNvvQbwOwBejPfaVx7DVuFO4AALeL4VXD0+gz6MiZkZtmoYvuzu37jJ1NcxYX70e0z2rchrv2YY3zHb+ClszXS+AeDfH5APd2BLCXgewEv99APA97H1cbCFrU86XwAwDuAJAK/1/o4dkB//HcALAM5jK9im++DHP8XWR9LzAJ7r/ftUv8ck4kdfxwTAR7BVxPU8tm4s/+Gma/YXAF4H8D8B5N/NdvULOiESgn5BJ0RCULALkRAU7EIkBAW7EAlBwS5EQlCwC5EQFOxCJAQFuxAJ4f8BRmRxF4R8V6YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.transpose(test_images[30], (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

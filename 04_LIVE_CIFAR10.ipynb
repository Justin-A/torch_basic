{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding = \"bytes\")\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = unpickle(\"../data/CIFAR10/cifar-10-batches-py/data_batch_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  43,  50, ..., 140,  84,  72],\n",
       "       [154, 126, 105, ..., 139, 142, 144],\n",
       "       [255, 253, 253, ...,  83,  83,  84],\n",
       "       ...,\n",
       "       [ 71,  60,  74, ...,  68,  69,  68],\n",
       "       [250, 254, 211, ..., 215, 255, 254],\n",
       "       [ 62,  61,  60, ..., 130, 130, 131]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[b'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[b'data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_to_images_and_labels(root):\n",
    "    data = unpickle(root)\n",
    "    data_images = data[b'data'] / 255\n",
    "    data_images = data_images.reshape(-1, 3, 32, 32).astype(\"float32\")\n",
    "    data_labels = data[b'labels']\n",
    "    return data_images, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "images1, label1 = pickle_to_images_and_labels(\"../data/CIFAR10/cifar-10-batches-py/data_batch_1\")\n",
    "images2, label2 = pickle_to_images_and_labels(\"../data/CIFAR10/cifar-10-batches-py/data_batch_2\")\n",
    "images3, label3 = pickle_to_images_and_labels(\"../data/CIFAR10/cifar-10-batches-py/data_batch_3\")\n",
    "images4, label4 = pickle_to_images_and_labels(\"../data/CIFAR10/cifar-10-batches-py/data_batch_4\")\n",
    "images5, label5 = pickle_to_images_and_labels(\"../data/CIFAR10/cifar-10-batches-py/data_batch_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = pickle_to_images_and_labels(\"../data/CIFAR10/cifar-10-batches-py/test_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_images = np.concatenate([images1, images2, images3, images4, images5], axis = 0)\n",
    "train_labels = np.concatenate([label1, label2, label3, label4, label5], axis = 0)\n",
    "test_images = np.concatenate([test_images], axis = 0)\n",
    "test_labels = np.concatenate([test_labels], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_images, valid_images, train_labels, valid_labels = train_test_split(train_images, train_labels, stratify = train_labels, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of Train Images:  (40000, 3, 32, 32)\n",
      "The Shape of Valid Images:  (10000, 3, 32, 32)\n",
      "The Shape of Test Images:  (10000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"The Shape of Train Images: \", train_images.shape)\n",
    "print(\"The Shape of Valid Images: \", valid_images.shape)\n",
    "print(\"The Shape of Test Images: \", test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Train Labels:  (40000,)\n",
      "The number of Valid Labels:  (10000,)\n",
      "The number of Test Labels:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of Train Labels: \", train_labels.shape)\n",
    "print(\"The number of Valid Labels: \", valid_labels.shape)\n",
    "print(\"The number of Test Labels: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_images_tensor = torch.tensor(train_images)\n",
    "train_labels_tensor = torch.tensor(train_labels)\n",
    "train_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "train_loader = DataLoader(train_tensor, batch_size = 64, num_workers = 0, shuffle = True)\n",
    "\n",
    "valid_images_tensor = torch.tensor(valid_images)\n",
    "valid_labels_tensor = torch.tensor(valid_labels)\n",
    "valid_tensor = TensorDataset(valid_images_tensor, valid_labels_tensor)\n",
    "valid_loader = DataLoader(valid_tensor, batch_size = 64, num_workers = 0, shuffle = True)\n",
    "\n",
    "test_images_tensor = torch.tensor(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, padding = 1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 16, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # 32 * 32 * 8\n",
    "        x = F.relu(x)     # 32 * 32 * 8\n",
    "        x = self.pool(x)  # 16 * 16 * 8\n",
    "        x = self.conv2(x) # 16 * 16 * 16\n",
    "        x = F.relu(x)     # 16 * 16 * 16\n",
    "        x = self.pool(x)  # 8 * 8 * 16\n",
    "        \n",
    "        x = x.view(-1, 8 * 8 * 16)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  CNN(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "model = CNN().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "print(\"Model: \", model)\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE, dtype = torch.int64)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            data, target= data.to(DEVICE), target.to(DEVICE, dtype = torch.int64)\n",
    "            output = model(data)\n",
    "            valid_loss += F.cross_entropy(output, target, reduction = \"sum\").item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "    \n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    valid_accuracy = 100. * correct / len(valid_loader.dataset)\n",
    "    return valid_loss, valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\venv\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/40000 (0%)]\tLoss: 1.361430\n",
      "Train Epoch: 1 [6400/40000 (16%)]\tLoss: 1.571870\n",
      "Train Epoch: 1 [12800/40000 (32%)]\tLoss: 1.260213\n",
      "Train Epoch: 1 [19200/40000 (48%)]\tLoss: 1.387439\n",
      "Train Epoch: 1 [25600/40000 (64%)]\tLoss: 1.346401\n",
      "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.231412\n",
      "Train Epoch: 1 [38400/40000 (96%)]\tLoss: 1.235862\n",
      "[EPOCH : 1], \tValidation Loss: 1.3872, \tValidation Accuracy: 50.24 % \n",
      "\n",
      "Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.260855\n",
      "Train Epoch: 2 [6400/40000 (16%)]\tLoss: 1.326415\n",
      "Train Epoch: 2 [12800/40000 (32%)]\tLoss: 1.168362\n",
      "Train Epoch: 2 [19200/40000 (48%)]\tLoss: 1.383433\n",
      "Train Epoch: 2 [25600/40000 (64%)]\tLoss: 1.057512\n",
      "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.182896\n",
      "Train Epoch: 2 [38400/40000 (96%)]\tLoss: 1.365272\n",
      "[EPOCH : 2], \tValidation Loss: 1.2979, \tValidation Accuracy: 54.10 % \n",
      "\n",
      "Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.111758\n",
      "Train Epoch: 3 [6400/40000 (16%)]\tLoss: 1.124160\n",
      "Train Epoch: 3 [12800/40000 (32%)]\tLoss: 1.288800\n",
      "Train Epoch: 3 [19200/40000 (48%)]\tLoss: 1.317991\n",
      "Train Epoch: 3 [25600/40000 (64%)]\tLoss: 1.303626\n",
      "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.257480\n",
      "Train Epoch: 3 [38400/40000 (96%)]\tLoss: 1.243374\n",
      "[EPOCH : 3], \tValidation Loss: 1.2769, \tValidation Accuracy: 54.44 % \n",
      "\n",
      "Train Epoch: 4 [0/40000 (0%)]\tLoss: 1.075140\n",
      "Train Epoch: 4 [6400/40000 (16%)]\tLoss: 1.221817\n",
      "Train Epoch: 4 [12800/40000 (32%)]\tLoss: 1.015497\n",
      "Train Epoch: 4 [19200/40000 (48%)]\tLoss: 1.273869\n",
      "Train Epoch: 4 [25600/40000 (64%)]\tLoss: 1.118431\n",
      "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 1.242788\n",
      "Train Epoch: 4 [38400/40000 (96%)]\tLoss: 1.138995\n",
      "[EPOCH : 4], \tValidation Loss: 1.2796, \tValidation Accuracy: 54.04 % \n",
      "\n",
      "Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.427469\n",
      "Train Epoch: 5 [6400/40000 (16%)]\tLoss: 1.096736\n",
      "Train Epoch: 5 [12800/40000 (32%)]\tLoss: 1.022228\n",
      "Train Epoch: 5 [19200/40000 (48%)]\tLoss: 1.115965\n",
      "Train Epoch: 5 [25600/40000 (64%)]\tLoss: 1.164093\n",
      "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.390425\n",
      "Train Epoch: 5 [38400/40000 (96%)]\tLoss: 1.096595\n",
      "[EPOCH : 5], \tValidation Loss: 1.2387, \tValidation Accuracy: 55.91 % \n",
      "\n",
      "Train Epoch: 6 [0/40000 (0%)]\tLoss: 1.224247\n",
      "Train Epoch: 6 [6400/40000 (16%)]\tLoss: 1.122560\n",
      "Train Epoch: 6 [12800/40000 (32%)]\tLoss: 1.221642\n",
      "Train Epoch: 6 [19200/40000 (48%)]\tLoss: 1.102595\n",
      "Train Epoch: 6 [25600/40000 (64%)]\tLoss: 1.199061\n",
      "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 1.302431\n",
      "Train Epoch: 6 [38400/40000 (96%)]\tLoss: 1.353957\n",
      "[EPOCH : 6], \tValidation Loss: 1.2223, \tValidation Accuracy: 56.81 % \n",
      "\n",
      "Train Epoch: 7 [0/40000 (0%)]\tLoss: 1.161115\n",
      "Train Epoch: 7 [6400/40000 (16%)]\tLoss: 1.216110\n",
      "Train Epoch: 7 [12800/40000 (32%)]\tLoss: 0.967422\n",
      "Train Epoch: 7 [19200/40000 (48%)]\tLoss: 1.255415\n",
      "Train Epoch: 7 [25600/40000 (64%)]\tLoss: 0.934658\n",
      "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 1.114909\n",
      "Train Epoch: 7 [38400/40000 (96%)]\tLoss: 1.041637\n",
      "[EPOCH : 7], \tValidation Loss: 1.1720, \tValidation Accuracy: 58.70 % \n",
      "\n",
      "Train Epoch: 8 [0/40000 (0%)]\tLoss: 1.137133\n",
      "Train Epoch: 8 [6400/40000 (16%)]\tLoss: 1.048262\n",
      "Train Epoch: 8 [12800/40000 (32%)]\tLoss: 1.130433\n",
      "Train Epoch: 8 [19200/40000 (48%)]\tLoss: 0.743048\n",
      "Train Epoch: 8 [25600/40000 (64%)]\tLoss: 1.118380\n",
      "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 1.303397\n",
      "Train Epoch: 8 [38400/40000 (96%)]\tLoss: 0.887488\n",
      "[EPOCH : 8], \tValidation Loss: 1.1533, \tValidation Accuracy: 59.56 % \n",
      "\n",
      "Train Epoch: 9 [0/40000 (0%)]\tLoss: 1.180444\n",
      "Train Epoch: 9 [6400/40000 (16%)]\tLoss: 1.049773\n",
      "Train Epoch: 9 [12800/40000 (32%)]\tLoss: 0.980881\n",
      "Train Epoch: 9 [19200/40000 (48%)]\tLoss: 1.017157\n",
      "Train Epoch: 9 [25600/40000 (64%)]\tLoss: 0.960528\n",
      "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 1.220239\n",
      "Train Epoch: 9 [38400/40000 (96%)]\tLoss: 0.947019\n",
      "[EPOCH : 9], \tValidation Loss: 1.1328, \tValidation Accuracy: 60.19 % \n",
      "\n",
      "Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.898633\n",
      "Train Epoch: 10 [6400/40000 (16%)]\tLoss: 1.141634\n",
      "Train Epoch: 10 [12800/40000 (32%)]\tLoss: 0.852636\n",
      "Train Epoch: 10 [19200/40000 (48%)]\tLoss: 1.075439\n",
      "Train Epoch: 10 [25600/40000 (64%)]\tLoss: 1.237481\n",
      "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 1.139879\n",
      "Train Epoch: 10 [38400/40000 (96%)]\tLoss: 0.973299\n",
      "[EPOCH : 10], \tValidation Loss: 1.1565, \tValidation Accuracy: 58.96 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''TRAINING'''\n",
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer)\n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_loader)\n",
    "    print(\"[EPOCH : {}], \\tValidation Loss: {:.4f}, \\tValidation Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, valid_loss, valid_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testset_prediction(model, test_images_tensor):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_images_tensor:\n",
    "            data = data.to(DEVICE)\n",
    "            output = model(data.view(-1, 3, 32, 32))\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            result.append(prediction.tolist())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justin\\venv\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5898"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test_predict_result = testset_prediction(model, test_images_tensor)\n",
    "accuracy_score(test_labels, np.squeeze(test_predict_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32, 32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images[30].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.transpose(test_images[30], (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_result[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
